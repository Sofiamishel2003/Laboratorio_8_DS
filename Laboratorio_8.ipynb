{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a5f73d",
   "metadata": {},
   "source": [
    "# Universidad del Valle de Guatemala\n",
    "\n",
    "## Facultad de Ingeniería\n",
    "\n",
    "### Departamento de Ciencias de la Computación\n",
    "\n",
    "**Laboratorio 8 – Detección de Anomalías con Autoencoder, Isolation Forest y LOF**\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "* José Rodrigo Marchena – 22398\n",
    "* Sofía Velasquez – 22049\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Carga del conjunto de datos\n",
    "\n",
    "Usaremos **CoverType** de UCI (vía `sklearn.datasets.fetch_covtype`), que contiene **581,012 observaciones** y **54 características** (10 numéricas continuas + 44 binarias: 4 de `Wilderness_Area` y 40 de `Soil_Type`). Mantendremos las binarias y **escalaremos solo las numéricas**, tal como se pide.  \n",
    "\n",
    "> Variables (resumen): Elevation, Aspect, Slope, distancias a hidrología/carreteras/fuego, Hillshade (9am/noon/3pm), 4 áreas silvestres binarias y 40 tipos de suelo binarios; la **etiqueta** `Cover_Type` tiene 7 clases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76aee28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (581012, 54) Shape y: (581012,)\n",
      "Primeras columnas: ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1']\n",
      "Numéricas detectadas: 10 -> ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
      "Binarias (Wilderness_Area): 4\n",
      "Binarias (Soil_Type): 40\n",
      "Total columnas: 54 (esperado 54)\n",
      "Valores únicos en algunas binarias: {'Wilderness_Area_0': array([1, 0]), 'Wilderness_Area_1': array([0, 1]), 'Wilderness_Area_2': array([0, 1])}\n",
      "Distribución Cover_Type (1..7):\n",
      " Cover_Type\n",
      "1    211840\n",
      "2    283301\n",
      "3     35754\n",
      "4      2747\n",
      "5      9493\n",
      "6     17367\n",
      "7     20510\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "# Cargar datos (X: 54 features, y: cover type [1..7])\n",
    "cov = fetch_covtype(as_frame=True)\n",
    "X_raw: pd.DataFrame = cov.data.copy()\n",
    "y_raw: pd.Series = cov.target.copy()  # 1..7 (int)\n",
    "\n",
    "print(\"Shape X:\", X_raw.shape, \"Shape y:\", y_raw.shape)\n",
    "print(\"Primeras columnas:\", X_raw.columns[:12].tolist())\n",
    "\n",
    "# Detectar columnas binarias por prefijo para evitar errores de nomenclatura\n",
    "wild_cols = [c for c in X_raw.columns if c.startswith(\"Wilderness_Area\")]\n",
    "soil_cols = [c for c in X_raw.columns if c.startswith(\"Soil_Type\")]\n",
    "bin_cols  = wild_cols + soil_cols\n",
    "\n",
    "# El resto de columnas se consideran numéricas\n",
    "num_cols = [c for c in X_raw.columns if c not in bin_cols]\n",
    "\n",
    "# Validaciones rápidas\n",
    "print(f\"Numéricas detectadas: {len(num_cols)} -> {num_cols}\")\n",
    "print(f\"Binarias (Wilderness_Area): {len(wild_cols)}\")\n",
    "print(f\"Binarias (Soil_Type): {len(soil_cols)}\")\n",
    "print(f\"Total columnas: {len(num_cols)+len(bin_cols)} (esperado 54)\")\n",
    "\n",
    "# (Opcional) Asegurar que binarios sean 0/1 (aunque vengan como float)\n",
    "X_raw[bin_cols] = X_raw[bin_cols].astype(int)\n",
    "print(\"Valores únicos en algunas binarias:\", {c: X_raw[c].unique()[:3] for c in bin_cols[:3]})\n",
    "\n",
    "# Distribución de Cover_Type\n",
    "print(\"Distribución Cover_Type (1..7):\\n\", y_raw.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949211b",
   "metadata": {},
   "source": [
    "## 2) Etiquetado: normal vs anómalo\n",
    "\n",
    "Normal = Cover_Type == 2 (Lodgepole Pine) y Anómalo = Cover_Type ≠ 2. Creamos y_binary en {0,1} con 1 = normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6794b285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporción de normales: 0.4876\n",
      "Conteos 0/1:\n",
      " Cover_Type\n",
      "0    297711\n",
      "1    283301\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_binary = (y_raw == 2).astype(int)  # 1=normal (Lodgepole Pine), 0=anómalo (resto)\n",
    "\n",
    "print(\"Proporción de normales:\", y_binary.mean().round(4))\n",
    "print(\"Conteos 0/1:\\n\", y_binary.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce916e1a",
   "metadata": {},
   "source": [
    "## 3) División Train / Validation / Test\n",
    "\n",
    "Para **Autoencoders**, entrenar y validar **solo con normales**; el **Test** lleva mezcla normal+anómalo para evaluar en un escenario realista. \n",
    "\n",
    "Propuesta:\n",
    "\n",
    "* **Test**: 15% estratificado (mezcla).\n",
    "* Del 85% restante, filtrar **solo normales** y dividir en **Train (70%)** y **Val (30%)**.\n",
    "* **Escalar solo numéricas** con `StandardScaler`, **ajustando** el escalador **exclusivamente en Train (normales)**; aplicar a Val/Test; **binarias** pasan “as is”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d47cc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (post-preprocess):\n",
      "  Train: (168564, 54) | y_train (solo normales): [1]\n",
      "  Val  : (72242, 54) | y_val   (solo normales): [1]\n",
      "  Test : (87152, 54) | y_test (0/1 mezclado):\n",
      "Cover_Type\n",
      "0    44657\n",
      "1    42495\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 3.1) Separar Test (15%) estratificado por normal/anómalo\n",
    "idx_all = np.arange(len(X_raw))\n",
    "idx_trainval, idx_test = train_test_split(\n",
    "    idx_all, test_size=0.15, stratify=y_binary, random_state=RANDOM_STATE\n",
    ")\n",
    "X_trainval = X_raw.iloc[idx_trainval].reset_index(drop=True)\n",
    "y_trainval = y_binary.iloc[idx_trainval].reset_index(drop=True)\n",
    "\n",
    "X_test = X_raw.iloc[idx_test].reset_index(drop=True)\n",
    "y_test = y_binary.iloc[idx_test].reset_index(drop=True)\n",
    "\n",
    "# 3.2) De TrainVal, quedarnos solo con normales para Train y Val\n",
    "mask_normals_tv = (y_trainval == 1)\n",
    "X_trainval_normals = X_trainval.loc[mask_normals_tv].reset_index(drop=True)\n",
    "y_trainval_normals = y_trainval.loc[mask_normals_tv].reset_index(drop=True)\n",
    "\n",
    "# 3.3) Split Train (70%) y Val (30%) SOLO con normales\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval_normals, y_trainval_normals, test_size=0.30,\n",
    "    random_state=RANDOM_STATE, shuffle=True\n",
    ")\n",
    "\n",
    "# 3.4) Preprocesamiento: escalar solo numéricas; binarias pasan tal cual\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(with_mean=True, with_std=True), num_cols),\n",
    "        (\"bin\", \"passthrough\", bin_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Ajustar con TRAIN (normales)\n",
    "preprocess.fit(X_train)\n",
    "\n",
    "# Transformar splits\n",
    "X_train_prep = preprocess.transform(X_train)\n",
    "X_val_prep   = preprocess.transform(X_val)\n",
    "X_test_prep  = preprocess.transform(X_test)\n",
    "\n",
    "print(\"Shapes (post-preprocess):\")\n",
    "print(\"  Train:\", X_train_prep.shape, \"| y_train (solo normales):\", np.unique(y_train))\n",
    "print(\"  Val  :\", X_val_prep.shape,   \"| y_val   (solo normales):\", np.unique(y_val))\n",
    "print(\"  Test :\", X_test_prep.shape,  \"| y_test (0/1 mezclado):\")\n",
    "print(pd.Series(y_test).value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
